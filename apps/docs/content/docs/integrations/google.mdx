---
title: Google AI Integration
description: Monitor Gemini models with LLMonitor
---

import { Tab, Tabs } from "fumadocs-ui/components/tabs";
import { Step, Steps } from "fumadocs-ui/components/steps";

LLMonitor provides complete observability for Google's Gemini models. Monitor costs, performance, and usage across all Gemini variants.

## Quick Setup

<Steps>

<Step>

### Install Dependencies

<Tabs items={['npm', 'yarn', 'pnpm']}>

<Tab value="npm">
  ```bash npm install @llmonitor/sdk @google/generative-ai ```
</Tab>

<Tab value="yarn">
  ```bash yarn add @llmonitor/sdk @google/generative-ai ```
</Tab>

<Tab value="pnpm">
  ```bash pnpm add @llmonitor/sdk @google/generative-ai ```
</Tab>

</Tabs>

</Step>

<Step>

### Basic Integration

```typescript
import { LLMonitor } from "@llmonitor/sdk";
import { GoogleGenerativeAI } from "@google/generative-ai";

const monitor = new LLMonitor({
  apiKey: "your-llmonitor-key",
});

const genAI = new GoogleGenerativeAI("your-google-api-key");
const model = genAI.getGenerativeModel({ model: "gemini-pro" });
const monitoredModel = monitor.google(model, "gemini-pro");

// Use exactly like the regular Google AI model
const response = await monitoredModel.generateContent("Hello Gemini!");
console.log(response.text());
```

</Step>

</Steps>

## Supported Models

| Model                 | Model ID            | Input Cost (1M tokens) | Output Cost (1M tokens) |
| --------------------- | ------------------- | ---------------------- | ----------------------- |
| **Gemini 1.5 Pro**    | `gemini-1.5-pro`    | $3.50                  | $10.50                  |
| **Gemini 1.5 Flash**  | `gemini-1.5-flash`  | $0.35                  | $1.05                   |
| **Gemini Pro**        | `gemini-pro`        | $0.50                  | $1.50                   |
| **Gemini Pro Vision** | `gemini-pro-vision` | $0.25                  | $0.50                   |

## Usage Examples

### Basic Text Generation

```typescript
const response = await monitoredModel.generateContent(
  "What is the capital of France?"
);
console.log(response.text());
// Paris is the capital of France.
```

### Multi-turn Conversation

```typescript
const chat = monitoredModel.startChat({
  history: [
    {
      role: "user",
      parts: [{ text: "Hello, I'm learning about astronomy." }],
    },
    {
      role: "model",
      parts: [
        {
          text: "That's wonderful! Astronomy is a fascinating field. What would you like to learn about?",
        },
      ],
    },
  ],
});

const result = await chat.sendMessage("Tell me about black holes.");
console.log(result.response.text());
```

### Streaming Generation

```typescript
const response = await monitoredModel.generateContentStream(
  "Write a short story about space exploration."
);

for await (const chunk of response.stream) {
  const chunkText = chunk.text();
  process.stdout.write(chunkText);
}

const finalResponse = await response.response;
console.log("\n\nFinal response:", finalResponse.text());
```

### Vision Capabilities

```typescript
import fs from "fs";

const visionModel = genAI.getGenerativeModel({ model: "gemini-pro-vision" });
const monitoredVisionModel = monitor.google(visionModel, "gemini-pro-vision");

const imageData = fs.readFileSync("image.jpg");
const imageParts = [
  {
    inlineData: {
      data: imageData.toString("base64"),
      mimeType: "image/jpeg",
    },
  },
];

const response = await monitoredVisionModel.generateContent([
  "What do you see in this image?",
  ...imageParts,
]);

console.log(response.text());
```

### Advanced Configuration

```typescript
const model = genAI.getGenerativeModel({
  model: "gemini-pro",
  generationConfig: {
    candidateCount: 1,
    stopSequences: ["x"],
    maxOutputTokens: 200,
    temperature: 0.9,
  },
});

const monitoredModel = monitor.google(model, "gemini-pro");

const response = await monitoredModel.generateContent(
  "Generate creative ideas for..."
);
```

## Monitoring Features

### Automatic Tracking

All Gemini API calls are automatically logged with:

- **Costs**: Calculated using official Google AI pricing
- **Token Usage**: Input and output tokens
- **Latency**: Request duration in milliseconds
- **Model Info**: Model name and parameters
- **Error Handling**: Failed requests with error details
- **Streaming Support**: Full tracking for streaming responses

### Custom Metadata

Add custom metadata to track specific use cases:

```typescript
const response = await monitoredModel.generateContent(
  "Analyze this business document.",
  {
    // LLMonitor options
    sessionId: "business-analysis-session",
    versionTag: "v1.2.0",
    metadata: {
      documentType: "financial",
      userId: "user-456",
      feature: "document-analysis",
    },
  }
);
```

## Error Handling

The wrapper preserves all original Google AI errors:

```typescript
try {
  const response = await monitoredModel.generateContent("Hello!");
  console.log(response.text());
} catch (error) {
  console.error("Google AI Error:", error.message);

  if (error.message.includes("API_KEY_INVALID")) {
    console.error("Check your Google AI API key");
  } else if (error.message.includes("RATE_LIMIT_EXCEEDED")) {
    console.error("Rate limit exceeded, implement backoff");
  }
}
```

## Performance Optimization

### Cost Optimization

1. **Choose the right model for your task**:

   - Gemini 1.5 Flash for fast, simple tasks
   - Gemini Pro for general-purpose use
   - Gemini 1.5 Pro for complex reasoning
   - Gemini Pro Vision for image analysis

2. **Optimize token usage**:
   ```typescript
   // Monitor token usage in LLMonitor dashboard
   const response = await monitoredModel.generateContent(
     "Provide a brief summary.", // Clear, concise prompts
     {
       generationConfig: {
         maxOutputTokens: 100, // Limit output tokens
         temperature: 0.1, // Lower temperature for focused responses
       },
     }
   );
   ```

### Prompt Engineering

```typescript
// Good: Structured prompts with clear instructions
const prompt = `
Task: Summarize the following article
Instructions: 
- Maximum 3 sentences
- Focus on key insights
- Use bullet points

Article: [article content here]
`;

const response = await monitoredModel.generateContent(prompt);

// Track prompt performance with version tags
const response = await monitoredModel.generateContent(optimizedPrompt, {
  versionTag: "prompt-v4",
  metadata: { promptType: "article-summary" },
});
```

## Production Configuration

### Environment Setup

```typescript
const monitor = new LLMonitor({
  apiKey: process.env.LLMONITOR_API_KEY!,
  enabled: process.env.NODE_ENV === "production",
  debug: process.env.NODE_ENV === "development",
});

const genAI = new GoogleGenerativeAI(process.env.GOOGLE_AI_API_KEY!);
const model = genAI.getGenerativeModel({ model: "gemini-pro" });
const monitoredModel = monitor.google(model, "gemini-pro");
```

### Batch Processing

```typescript
async function processBatch(prompts: string[]) {
  const results = [];

  for (const prompt of prompts) {
    try {
      const response = await monitoredModel.generateContent(prompt);

      results.push({
        input: prompt,
        output: response.text(),
        success: true,
      });
    } catch (error) {
      results.push({
        input: prompt,
        error: error.message,
        success: false,
      });
    }

    // Add delay to respect rate limits
    await new Promise((resolve) => setTimeout(resolve, 100));
  }

  return results;
}
```

### Rate Limiting

```typescript
class RateLimitedGemini {
  private lastCallTime = 0;
  private minInterval = 100; // 100ms between calls

  async generateContent(prompt: string) {
    const now = Date.now();
    const timeSinceLastCall = now - this.lastCallTime;

    if (timeSinceLastCall < this.minInterval) {
      await new Promise((resolve) =>
        setTimeout(resolve, this.minInterval - timeSinceLastCall)
      );
    }

    this.lastCallTime = Date.now();
    return await monitoredModel.generateContent(prompt);
  }
}
```

## React Integration

Use with the React package for component-level tracking:

```jsx
import { useLLMonitor } from "@llmonitor/react";
import { GoogleGenerativeAI } from "@google/generative-ai";

function AIContentGenerator() {
  const { track } = useLLMonitor();
  const [content, setContent] = useState("");

  const generateContent = async (prompt) => {
    const result = await track("gemini-generation", async () => {
      const genAI = new GoogleGenerativeAI(process.env.GOOGLE_AI_API_KEY);
      const model = genAI.getGenerativeModel({ model: "gemini-pro" });

      const response = await model.generateContent(prompt);
      return response.text();
    });

    setContent(result);
  };

  return (
    <div>
      <button onClick={() => generateContent("Write a haiku about coding")}>
        Generate Haiku
      </button>
      <p>{content}</p>
    </div>
  );
}
```

## Streaming Integration

```jsx
function StreamingChat() {
  const [messages, setMessages] = useState([]);
  const [currentMessage, setCurrentMessage] = useState("");

  const sendStreamingMessage = async (userMessage) => {
    setMessages((prev) => [...prev, { role: "user", content: userMessage }]);
    setCurrentMessage("");

    const response = await monitoredModel.generateContentStream(userMessage);

    for await (const chunk of response.stream) {
      const chunkText = chunk.text();
      setCurrentMessage((prev) => prev + chunkText);
    }

    const finalResponse = await response.response;
    setMessages((prev) => [
      ...prev,
      { role: "assistant", content: finalResponse.text() },
    ]);
    setCurrentMessage("");
  };

  return (
    <div>
      {messages.map((msg, i) => (
        <div key={i}>
          {msg.role}: {msg.content}
        </div>
      ))}
      {currentMessage && <div>Assistant: {currentMessage}</div>}
    </div>
  );
}
```

## Troubleshooting

### Common Issues

1. **API Key Issues**

   ```typescript
   // Verify your Google AI API key
   const genAI = new GoogleGenerativeAI("your-api-key");
   try {
     const model = genAI.getGenerativeModel({ model: "gemini-pro" });
     await model.generateContent("test");
   } catch (error) {
     console.error("API Key issue:", error.message);
   }
   ```

2. **Model Availability**

   ```typescript
   // Check if the model is available in your region
   const availableModels = [
     "gemini-pro",
     "gemini-pro-vision",
     "gemini-1.5-pro",
   ];

   for (const modelName of availableModels) {
     try {
       const model = genAI.getGenerativeModel({ model: modelName });
       await model.generateContent("test");
       console.log(`${modelName} is available`);
     } catch (error) {
       console.log(`${modelName} is not available: ${error.message}`);
     }
   }
   ```

3. **Content Filtering**

   ```typescript
   // Handle content filtering responses
   try {
     const response = await monitoredModel.generateContent(prompt);

     if (response.response.promptFeedback?.blockReason) {
       console.log(
         "Content was blocked:",
         response.response.promptFeedback.blockReason
       );
     }

     return response.text();
   } catch (error) {
     if (error.message.includes("SAFETY")) {
       console.log("Content filtered for safety reasons");
     }
     throw error;
   }
   ```

4. **Token Limits**

   ```typescript
   // Monitor token usage to avoid limits
   const response = await monitoredModel.generateContent(prompt);

   // Check token count in LLMonitor dashboard
   console.log("Tokens used:", response.response.usageMetadata);
   ```

## Next Steps

- [Cost Optimization Guide](/guides/cost-optimization)
- [Streaming Best Practices](/guides/streaming)
- [React Integration](/integrations/react)
- [Vision Model Examples](/examples/vision)
