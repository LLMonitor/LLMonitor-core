---
title: OpenAI Integration
description: Complete guide to integrate LLMonitor with OpenAI models
---

import { Tab, Tabs } from "fumadocs-ui/components/tabs";
import { Step, Steps } from "fumadocs-ui/components/steps";
import { Callout } from "fumadocs-ui/components/callout";

# OpenAI Integration

LLMonitor provides seamless integration with OpenAI's GPT models, including GPT-4, GPT-3.5 Turbo, DALL-E, and Whisper. Track costs, performance, and usage patterns across all your OpenAI API calls.

## Quick Start

<Steps>

<Step>

### Install Dependencies

<Tabs items={['npm', 'yarn', 'pnpm', 'bun']}>

<Tab value="npm">```bash npm install openai @llmonitor/sdk ```</Tab>

<Tab value="yarn">```bash yarn add openai @llmonitor/sdk ```</Tab>

<Tab value="pnpm">```bash pnpm add openai @llmonitor/sdk ```</Tab>

<Tab value="bun">```bash bun add openai @llmonitor/sdk ```</Tab>

</Tabs>

</Step>

<Step>

### Basic Setup

```typescript
import OpenAI from "openai";
import { LLMonitor } from "@llmonitor/sdk";

// Initialize OpenAI client
const openai = new OpenAI({
  apiKey: process.env.OPENAI_API_KEY,
});

// Initialize LLMonitor
const monitor = new LLMonitor({
  apiKey: process.env.LLMONITOR_API_KEY,
  sessionId: "user-123", // Optional: track user sessions
});

// Wrap OpenAI client with monitoring
const monitoredOpenAI = monitor.openai(openai);
```

</Step>

<Step>

### Make Your First Monitored Call

```typescript
async function example() {
  const response = await monitoredOpenAI.chat.completions.create({
    model: "gpt-4",
    messages: [
      { role: "system", content: "You are a helpful assistant." },
      { role: "user", content: "Hello! How are you?" },
    ],
    max_tokens: 150,
    temperature: 0.7,
  });

  console.log(response.choices[0]?.message.content);
  // This call is now automatically logged to LLMonitor!
}
```

</Step>

</Steps>

## Supported Models

LLMonitor supports all OpenAI models and automatically calculates costs:

### Chat Completions

| Model               | Description              | Use Case                    |
| ------------------- | ------------------------ | --------------------------- |
| `gpt-4`             | Most capable model       | Complex reasoning, analysis |
| `gpt-4-turbo`       | Faster and cheaper GPT-4 | Balanced performance/cost   |
| `gpt-3.5-turbo`     | Fast and efficient       | Simple tasks, high volume   |
| `gpt-3.5-turbo-16k` | Extended context length  | Long documents              |

### Other Models

- **DALL-E 3** - Image generation
- **DALL-E 2** - Image generation (cheaper)
- **Whisper** - Speech to text
- **TTS** - Text to speech
- **GPT-4 Vision** - Multimodal understanding

## Features

### Automatic Cost Tracking

LLMonitor automatically calculates costs using OpenAI's latest pricing:

```typescript
// Costs are automatically tracked for all calls
const response = await monitoredOpenAI.chat.completions.create({
  model: "gpt-4",
  messages: [{ role: "user", content: "Explain quantum computing" }],
  max_tokens: 500,
});

// View costs in your LLMonitor dashboard
// - Input tokens: $0.03 per 1K tokens
// - Output tokens: $0.06 per 1K tokens
// - Total cost calculated automatically
```

### Session Tracking

Track conversations and user sessions:

```typescript
// Track user sessions
const userMonitor = new LLMonitor({
  apiKey: process.env.LLMONITOR_API_KEY,
  sessionId: `user-${userId}`,
  metadata: {
    userId: userId,
    plan: "premium",
  },
});

const monitoredOpenAI = userMonitor.openai(openai);

// All calls are now associated with this user session
```

### Performance Monitoring

Monitor latency, token usage, and success rates:

```typescript
// LLMonitor automatically tracks:
// - Response time
// - Input/output tokens
// - Success/failure rates
// - Model performance metrics

const startTime = Date.now();
const response = await monitoredOpenAI.chat.completions.create({
  model: "gpt-3.5-turbo",
  messages: [{ role: "user", content: "Quick question about AI" }],
});
// Response time automatically logged
```

## Advanced Configuration

### Custom Metadata

Add custom metadata to track additional context:

```typescript
// Add metadata to individual calls
const response = await monitoredOpenAI.chat.completions.create(
  {
    model: "gpt-4",
    messages: [{ role: "user", content: "Help with customer support" }],
  },
  {
    // LLMonitor metadata
    metadata: {
      category: "customer-support",
      priority: "high",
      department: "sales",
      requestId: "req-123",
    },
  }
);
```

### Error Handling and Retries

Monitor errors and implement retry logic:

```typescript
import { LLMonitorError } from "@llmonitor/sdk";

async function robustOpenAICall(prompt: string, retries = 3) {
  for (let attempt = 1; attempt <= retries; attempt++) {
    try {
      const response = await monitoredOpenAI.chat.completions.create({
        model: "gpt-4",
        messages: [{ role: "user", content: prompt }],
      });

      return response;
    } catch (error) {
      console.log(`Attempt ${attempt} failed:`, error.message);

      if (attempt === retries) {
        // LLMonitor automatically logs this error
        throw new LLMonitorError("Max retries reached", error);
      }

      // Wait before retry
      await new Promise((resolve) => setTimeout(resolve, 1000 * attempt));
    }
  }
}
```

### Streaming Support

Monitor streaming responses:

```typescript
// Streaming is fully supported
const stream = await monitoredOpenAI.chat.completions.create({
  model: "gpt-4",
  messages: [{ role: "user", content: "Tell me a story" }],
  stream: true,
});

let fullResponse = "";
for await (const chunk of stream) {
  const content = chunk.choices[0]?.delta?.content || "";
  fullResponse += content;
  process.stdout.write(content);
}

// LLMonitor automatically tracks the complete streaming response
```

## Model-Specific Examples

### GPT-4 for Complex Analysis

```typescript
async function complexAnalysis(document: string) {
  const response = await monitoredOpenAI.chat.completions.create({
    model: "gpt-4",
    messages: [
      {
        role: "system",
        content:
          "You are an expert business analyst. Provide detailed, actionable insights.",
      },
      {
        role: "user",
        content: `Analyze this business document and provide key insights:\n\n${document}`,
      },
    ],
    max_tokens: 1000,
    temperature: 0.3, // Lower temperature for analysis
  });

  return response.choices[0]?.message.content;
}
```

### GPT-3.5 for High-Volume Tasks

```typescript
async function classifyEmails(emails: string[]) {
  const classifications = [];

  for (const email of emails) {
    const response = await monitoredOpenAI.chat.completions.create({
      model: "gpt-3.5-turbo",
      messages: [
        {
          role: "system",
          content:
            "Classify emails as 'spam', 'important', or 'normal'. Respond with one word only.",
        },
        { role: "user", content: email },
      ],
      max_tokens: 5,
      temperature: 0,
    });

    classifications.push(response.choices[0]?.message.content?.toLowerCase());
  }

  return classifications;
}
```

### DALL-E Image Generation

```typescript
async function generateImage(prompt: string) {
  // LLMonitor supports DALL-E monitoring
  const response = await monitoredOpenAI.images.generate({
    model: "dall-e-3",
    prompt: prompt,
    size: "1024x1024",
    quality: "standard",
    n: 1,
  });

  return response.data[0]?.url;
  // Cost automatically calculated based on image size and quality
}
```

### Whisper Speech-to-Text

```typescript
async function transcribeAudio(audioFile: File) {
  const response = await monitoredOpenAI.audio.transcriptions.create({
    model: "whisper-1",
    file: audioFile,
    language: "en", // Optional: specify language
  });

  return response.text;
  // Duration and cost automatically tracked
}
```

## Function Calling

Monitor OpenAI function calling:

```typescript
const functions = [
  {
    name: "get_weather",
    description: "Get current weather for a location",
    parameters: {
      type: "object",
      properties: {
        location: { type: "string" },
        unit: { type: "string", enum: ["celsius", "fahrenheit"] },
      },
      required: ["location"],
    },
  },
];

async function chatWithFunctions(message: string) {
  const response = await monitoredOpenAI.chat.completions.create({
    model: "gpt-4",
    messages: [{ role: "user", content: message }],
    functions: functions,
    function_call: "auto",
  });

  const functionCall = response.choices[0]?.message.function_call;

  if (functionCall) {
    // LLMonitor tracks function calls and their parameters
    console.log("Function called:", functionCall.name);
    console.log("Arguments:", functionCall.arguments);
  }

  return response;
}
```

## Embeddings

Monitor embedding generation for RAG applications:

```typescript
async function generateEmbeddings(texts: string[]) {
  const response = await monitoredOpenAI.embeddings.create({
    model: "text-embedding-ada-002",
    input: texts,
  });

  // LLMonitor tracks:
  // - Number of tokens processed
  // - Cost per embedding
  // - Batch size efficiency

  return response.data.map((item) => item.embedding);
}
```

## Best Practices

### Cost Optimization

```typescript
// Use appropriate models for different tasks
const taskModels = {
  classification: "gpt-3.5-turbo",
  analysis: "gpt-4-turbo",
  creative: "gpt-4",
  simple: "gpt-3.5-turbo",
};

async function smartModelSelection(task: string, prompt: string) {
  const model = taskModels[task] || "gpt-3.5-turbo";

  const response = await monitoredOpenAI.chat.completions.create({
    model: model,
    messages: [{ role: "user", content: prompt }],
    max_tokens: task === "classification" ? 10 : 500,
  });

  return response;
}
```

### Error Handling

```typescript
async function safeOpenAICall(prompt: string) {
  try {
    const response = await monitoredOpenAI.chat.completions.create({
      model: "gpt-4",
      messages: [{ role: "user", content: prompt }],
    });

    return response.choices[0]?.message.content;
  } catch (error) {
    // LLMonitor automatically logs errors with context
    console.error("OpenAI API error:", error);

    if (error.status === 429) {
      // Rate limit handling
      throw new Error("Rate limit exceeded. Please try again later.");
    } else if (error.status === 500) {
      // Server error - retry with exponential backoff
      throw new Error("OpenAI server error. Retrying...");
    }

    throw error;
  }
}
```

### Prompt Templates

Use templates for consistent monitoring:

```typescript
class PromptTemplate {
  constructor(private template: string) {}

  async execute(variables: Record<string, string>, model = "gpt-3.5-turbo") {
    let prompt = this.template;

    // Replace variables in template
    Object.entries(variables).forEach(([key, value]) => {
      prompt = prompt.replace(`{{${key}}}`, value);
    });

    const response = await monitoredOpenAI.chat.completions.create(
      {
        model: model,
        messages: [{ role: "user", content: prompt }],
      },
      {
        metadata: {
          template: this.template,
          variables: variables,
        },
      }
    );

    return response.choices[0]?.message.content;
  }
}

// Usage
const emailTemplate = new PromptTemplate(
  "Write a {{tone}} email about {{topic}} for {{audience}}"
);

const email = await emailTemplate.execute({
  tone: "professional",
  topic: "quarterly results",
  audience: "investors",
});
```

## Monitoring Dashboard

Your LLMonitor dashboard will show:

### Real-time Metrics

- **Requests per minute** - Current API usage
- **Average response time** - Performance monitoring
- **Error rate** - Success/failure tracking
- **Cost per hour** - Real-time spending

### Model Analytics

- **Model usage distribution** - Which models you use most
- **Cost by model** - Spending breakdown
- **Performance by model** - Response times and success rates
- **Token efficiency** - Tokens per request trends

### User Analytics (with session tracking)

- **Cost per user** - Individual user spending
- **Usage patterns** - When users are most active
- **Session analysis** - Conversation flow and length

## Troubleshooting

### Common Issues

#### 1. API Key Not Working

```typescript
// Verify your API key
const testMonitor = new LLMonitor({
  apiKey: process.env.LLMONITOR_API_KEY,
  debug: true, // Enable debug logs
});

// Check if key is valid
if (!process.env.LLMONITOR_API_KEY) {
  throw new Error("LLMONITOR_API_KEY environment variable is required");
}
```

#### 2. Calls Not Appearing in Dashboard

<Callout type="warn">
  Make sure you're using the monitored client (`monitoredOpenAI`) instead of the
  original client (`openai`).
</Callout>

```typescript
// ❌ Wrong - calls won't be monitored
const response = await openai.chat.completions.create({...});

// ✅ Correct - calls are monitored
const response = await monitoredOpenAI.chat.completions.create({...});
```

#### 3. High Latency

If you notice increased latency:

```typescript
// Enable async mode for fire-and-forget logging
const monitor = new LLMonitor({
  apiKey: process.env.LLMONITOR_API_KEY,
  async: true, // Reduces latency
  batchSize: 10, // Batch requests for efficiency
});
```

#### 4. Rate Limiting

Handle OpenAI rate limits gracefully:

```typescript
const response = await monitoredOpenAI.chat.completions.create(
  {
    model: "gpt-4",
    messages: [{ role: "user", content: prompt }],
  },
  {
    retries: 3,
    retryDelay: (attempt) => Math.pow(2, attempt) * 1000, // Exponential backoff
  }
);
```

## Next Steps

Now that you have OpenAI monitoring set up:

- [**Cost Optimization**](/docs/guides/cost-optimization) - Reduce your OpenAI costs
- [**A/B Testing**](/docs/guides/ab-testing) - Compare different prompts and models
- [**Session Tracking**](/docs/guides/sessions) - Track user conversations
- [**Performance Monitoring**](/docs/guides/performance) - Optimize response times

## Need Help?

- Check our [Troubleshooting Guide](/docs/guides/troubleshooting)
- Join our [Discord Community](https://discord.gg/llmonitor)
- View [OpenAI Examples](/docs/examples/openai) for more use cases
