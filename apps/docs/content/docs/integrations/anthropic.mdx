---
title: Anthropic Integration
description: Monitor Claude models with LLMonitor
---

import { Tab, Tabs } from "fumadocs-ui/components/tabs";
import { Step, Steps } from "fumadocs-ui/components/steps";

LLMonitor provides complete observability for Anthropic's Claude models. Monitor costs, performance, and usage across all Claude variants.

## Quick Setup

<Steps>

<Step>

### Install Dependencies

<Tabs items={['npm', 'yarn', 'pnpm']}>

<Tab value="npm">```bash npm install @llmonitor/sdk @anthropic-ai/sdk ```</Tab>

<Tab value="yarn">```bash yarn add @llmonitor/sdk @anthropic-ai/sdk ```</Tab>

<Tab value="pnpm">```bash pnpm add @llmonitor/sdk @anthropic-ai/sdk ```</Tab>

</Tabs>

</Step>

<Step>

### Basic Integration

```typescript
import { LLMonitor } from "@llmonitor/sdk";
import { Anthropic } from "@anthropic-ai/sdk";

const monitor = new LLMonitor({
  apiKey: "your-llmonitor-key",
});

const anthropic = monitor.anthropic(
  new Anthropic({
    apiKey: "your-anthropic-key",
  })
);

// Use exactly like the regular Anthropic client
const response = await anthropic.messages.create({
  model: "claude-3-sonnet-20240229",
  messages: [{ role: "user", content: "Hello Claude!" }],
  max_tokens: 150,
});
```

</Step>

</Steps>

## Supported Models

| Model                 | Model ID                     | Input Cost (1M tokens) | Output Cost (1M tokens) |
| --------------------- | ---------------------------- | ---------------------- | ----------------------- |
| **Claude-3 Opus**     | `claude-3-opus-20240229`     | $15.00                 | $75.00                  |
| **Claude-3.5 Sonnet** | `claude-3-5-sonnet-20241022` | $3.00                  | $15.00                  |
| **Claude-3 Sonnet**   | `claude-3-sonnet-20240229`   | $3.00                  | $15.00                  |
| **Claude-3 Haiku**    | `claude-3-haiku-20240307`    | $0.25                  | $1.25                   |

## Usage Examples

### Basic Chat

```typescript
const response = await anthropic.messages.create({
  model: "claude-3-sonnet-20240229",
  messages: [{ role: "user", content: "What is the capital of France?" }],
  max_tokens: 100,
});

console.log(response.content[0].text);
// Paris is the capital of France.
```

### Multi-turn Conversation

```typescript
const conversation = [
  { role: "user", content: "Hello Claude! I'm working on a project." },
  {
    role: "assistant",
    content:
      "Hello! I'd be happy to help with your project. What are you working on?",
  },
  { role: "user", content: "I need help with Python data analysis." },
];

const response = await anthropic.messages.create({
  model: "claude-3-sonnet-20240229",
  messages: conversation,
  max_tokens: 500,
});
```

### System Prompts

```typescript
const response = await anthropic.messages.create({
  model: "claude-3-sonnet-20240229",
  system: "You are a helpful assistant that always responds in JSON format.",
  messages: [{ role: "user", content: "What's the weather like?" }],
  max_tokens: 200,
});
```

### Advanced Configuration

```typescript
const response = await anthropic.messages.create({
  model: "claude-3-opus-20240229",
  messages: [{ role: "user", content: "Write a short story." }],
  max_tokens: 1000,
  temperature: 0.8,
  top_p: 0.9,
  stop_sequences: ["THE END"],
});
```

## Monitoring Features

### Automatic Tracking

All Claude API calls are automatically logged with:

- **Costs**: Calculated using official Anthropic pricing
- **Token Usage**: Input and output tokens
- **Latency**: Request duration in milliseconds
- **Model Info**: Model name and parameters
- **Error Handling**: Failed requests with error details

### Custom Metadata

Add custom metadata to track specific use cases:

```typescript
const response = await anthropic.messages.create(
  {
    model: "claude-3-sonnet-20240229",
    messages: [{ role: "user", content: "Analyze this document." }],
    max_tokens: 500,
  },
  {
    // LLMonitor options
    sessionId: "document-analysis-session",
    versionTag: "v2.1.0",
    metadata: {
      documentType: "legal",
      userId: "user-123",
      feature: "document-analysis",
    },
  }
);
```

## Error Handling

The wrapper preserves all original Anthropic errors:

```typescript
try {
  const response = await anthropic.messages.create({
    model: "claude-3-sonnet-20240229",
    messages: [{ role: "user", content: "Hello!" }],
    max_tokens: 150,
  });
} catch (error) {
  if (error instanceof Anthropic.APIError) {
    console.error("Anthropic API Error:", error.message);
    console.error("Status:", error.status);
  } else {
    console.error("Other error:", error);
  }
}
```

## Performance Optimization

### Cost Optimization

1. **Use the right model for the task**:

   - Claude-3 Haiku for simple tasks
   - Claude-3 Sonnet for balanced performance
   - Claude-3 Opus for complex reasoning

2. **Optimize token usage**:
   ```typescript
   // Monitor token usage in LLMonitor dashboard
   const response = await anthropic.messages.create({
     model: "claude-3-haiku-20240307", // Cheapest option
     messages: [{ role: "user", content: "Brief summary please." }],
     max_tokens: 100, // Limit output tokens
   });
   ```

### Prompt Engineering

```typescript
// Good: Clear, concise prompts
const response = await anthropic.messages.create({
  model: "claude-3-sonnet-20240229",
  messages: [
    {
      role: "user",
      content: "Summarize this article in 2 sentences: [article text]",
    },
  ],
  max_tokens: 100,
});

// Track prompt performance with version tags
const response = await anthropic.messages.create(
  {
    model: "claude-3-sonnet-20240229",
    messages: [{ role: "user", content: optimizedPrompt }],
    max_tokens: 150,
  },
  {
    versionTag: "prompt-v3",
    metadata: { promptType: "summarization" },
  }
);
```

## Production Configuration

### Environment Setup

```typescript
const monitor = new LLMonitor({
  apiKey: process.env.LLMONITOR_API_KEY!,
  enabled: process.env.NODE_ENV === "production",
  debug: process.env.NODE_ENV === "development",
});

const anthropic = monitor.anthropic(
  new Anthropic({
    apiKey: process.env.ANTHROPIC_API_KEY!,
  })
);
```

### Batch Processing

```typescript
async function processBatch(messages: string[]) {
  const results = [];

  for (const message of messages) {
    try {
      const response = await anthropic.messages.create({
        model: "claude-3-sonnet-20240229",
        messages: [{ role: "user", content: message }],
        max_tokens: 200,
      });

      results.push({
        input: message,
        output: response.content[0].text,
        success: true,
      });
    } catch (error) {
      results.push({
        input: message,
        error: error.message,
        success: false,
      });
    }
  }

  return results;
}
```

## React Integration

Use with the React package for component-level tracking:

```jsx
import { useLLMonitor } from "@llmonitor/react";
import { Anthropic } from "@anthropic-ai/sdk";

function ChatComponent() {
  const { track } = useLLMonitor();

  const handleMessage = async (message) => {
    await track("claude-chat", async () => {
      const anthropic = new Anthropic({
        apiKey: process.env.ANTHROPIC_API_KEY,
      });

      return await anthropic.messages.create({
        model: "claude-3-sonnet-20240229",
        messages: [{ role: "user", content: message }],
        max_tokens: 300,
      });
    });
  };

  return <div>{/* Your chat UI */}</div>;
}
```

## Troubleshooting

### Common Issues

1. **API Key Issues**

   ```typescript
   // Check if your Anthropic API key is valid
   const anthropic = new Anthropic({ apiKey: "your-key" });
   try {
     await anthropic.messages.create({
       /* ... */
     });
   } catch (error) {
     console.error("API Key issue:", error.message);
   }
   ```

2. **Rate Limiting**

   ```typescript
   // Implement exponential backoff
   async function callWithRetry(params, maxRetries = 3) {
     for (let i = 0; i < maxRetries; i++) {
       try {
         return await anthropic.messages.create(params);
       } catch (error) {
         if (error.status === 429 && i < maxRetries - 1) {
           await new Promise((resolve) =>
             setTimeout(resolve, 1000 * Math.pow(2, i))
           );
           continue;
         }
         throw error;
       }
     }
   }
   ```

3. **Memory Usage**
   ```typescript
   // For long conversations, manage context window
   function truncateConversation(messages, maxTokens = 4000) {
     // Implement token counting and truncation logic
     return messages.slice(-10); // Keep last 10 messages
   }
   ```

## Next Steps

- [Cost Optimization Guide](/guides/cost-optimization)
- [A/B Testing with Claude](/guides/ab-testing)
- [React Integration](/integrations/react)
- [Express.js Middleware](/integrations/express)
