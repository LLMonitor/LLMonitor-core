---
title: Express.js Integration
description: Monitor LLM calls in Express.js applications
---

import { Tab, Tabs } from "fumadocs-ui/components/tabs";
import { Step, Steps } from "fumadocs-ui/components/steps";

LLMonitor provides Express.js middleware for automatic request tracking and LLM monitoring in your Node.js backend applications.

## Quick Setup

<Steps>

<Step>

### Install Dependencies

<Tabs items={['npm', 'yarn', 'pnpm']}>

<Tab value="npm">```bash npm install @llmonitor/sdk express ```</Tab>

<Tab value="yarn">```bash yarn add @llmonitor/sdk express ```</Tab>

<Tab value="pnpm">```bash pnpm add @llmonitor/sdk express ```</Tab>

</Tabs>

</Step>

<Step>

### Add Middleware

```javascript
import express from "express";
import { createExpressMiddleware } from "@llmonitor/sdk";

const app = express();

// Add LLMonitor middleware
app.use(
  createExpressMiddleware({
    apiKey: process.env.LLMONITOR_API_KEY,
    skipPaths: ["/health", "/metrics"],
    extractSessionId: (req) => req.headers["x-session-id"],
    extractMetadata: (req) => ({
      userAgent: req.headers["user-agent"],
      ip: req.ip,
      path: req.path,
    }),
  })
);

app.listen(3000);
```

</Step>

<Step>

### Use LLM Providers

```javascript
import { LLMonitor } from "@llmonitor/sdk";
import OpenAI from "openai";

const monitor = new LLMonitor({
  apiKey: process.env.LLMONITOR_API_KEY,
});

const openai = monitor.openai(
  new OpenAI({
    apiKey: process.env.OPENAI_API_KEY,
  })
);

app.post("/chat", async (req, res) => {
  const { message } = req.body;

  const response = await openai.chat.completions.create({
    model: "gpt-4",
    messages: [{ role: "user", content: message }],
  });

  res.json({ reply: response.choices[0].message.content });
});
```

</Step>

</Steps>

## Middleware Configuration

### Basic Configuration

```javascript
import { createExpressMiddleware } from "@llmonitor/sdk";

const llmonitorMiddleware = createExpressMiddleware({
  apiKey: "your-api-key",

  // Optional: Skip certain paths
  skipPaths: ["/health", "/metrics", "/favicon.ico"],

  // Optional: Skip based on request method
  skipMethods: ["OPTIONS"],

  // Optional: Custom session extraction
  extractSessionId: (req) => {
    return req.session?.userId || req.headers["x-session-id"];
  },

  // Optional: Custom metadata extraction
  extractMetadata: (req) => ({
    userAgent: req.headers["user-agent"],
    referer: req.headers.referer,
    ip: req.ip || req.connection.remoteAddress,
    method: req.method,
    path: req.path,
    query: req.query,
  }),

  // Optional: Enable/disable middleware
  enabled: process.env.NODE_ENV === "production",
});

app.use(llmonitorMiddleware);
```

### Advanced Configuration

```javascript
const llmonitorMiddleware = createExpressMiddleware({
  apiKey: process.env.LLMONITOR_API_KEY,

  // Skip paths using regex or functions
  skipPaths: [
    "/health",
    /^\/api\/internal/,
    (req) => req.path.startsWith("/admin") && !req.user?.isAdmin,
  ],

  // Custom error handling
  onError: (error, req, res) => {
    console.error("LLMonitor middleware error:", error);
    // Don't interrupt the request flow
  },

  // Custom request/response logging
  logRequest: true,
  logResponse: true,

  // Rate limiting for monitoring
  rateLimitPerMinute: 1000,

  // Custom session management
  sessionManager: {
    extract: (req) => req.session?.id,
    create: () => `session-${Date.now()}-${Math.random()}`,
    store: (req, sessionId) => {
      req.session = req.session || {};
      req.session.llmonitorId = sessionId;
    },
  },
});
```

## Complete API Example

```javascript
import express from "express";
import { LLMonitor, createExpressMiddleware } from "@llmonitor/sdk";
import OpenAI from "openai";
import { Anthropic } from "@anthropic-ai/sdk";

const app = express();
app.use(express.json());

// Initialize LLMonitor
const monitor = new LLMonitor({
  apiKey: process.env.LLMONITOR_API_KEY,
  debug: process.env.NODE_ENV === "development",
});

// Add middleware
app.use(
  createExpressMiddleware({
    apiKey: process.env.LLMONITOR_API_KEY,
    extractSessionId: (req) => req.session?.userId,
    extractMetadata: (req) => ({
      endpoint: req.path,
      userAgent: req.headers["user-agent"],
    }),
  })
);

// Initialize LLM clients
const openai = monitor.openai(
  new OpenAI({
    apiKey: process.env.OPENAI_API_KEY,
  })
);

const anthropic = monitor.anthropic(
  new Anthropic({
    apiKey: process.env.ANTHROPIC_API_KEY,
  })
);

// Chat endpoint with OpenAI
app.post("/api/chat/openai", async (req, res) => {
  try {
    const { message, model = "gpt-4" } = req.body;

    const response = await openai.chat.completions.create({
      model,
      messages: [{ role: "user", content: message }],
      temperature: 0.7,
      max_tokens: 500,
    });

    res.json({
      reply: response.choices[0].message.content,
      model,
      usage: response.usage,
    });
  } catch (error) {
    res.status(500).json({ error: error.message });
  }
});

// Chat endpoint with Claude
app.post("/api/chat/claude", async (req, res) => {
  try {
    const { message, model = "claude-3-sonnet-20240229" } = req.body;

    const response = await anthropic.messages.create({
      model,
      messages: [{ role: "user", content: message }],
      max_tokens: 500,
    });

    res.json({
      reply: response.content[0].text,
      model,
      usage: response.usage,
    });
  } catch (error) {
    res.status(500).json({ error: error.message });
  }
});

// Conversation endpoint
app.post("/api/conversation", async (req, res) => {
  try {
    const { messages, provider = "openai" } = req.body;

    let response;
    if (provider === "openai") {
      response = await openai.chat.completions.create({
        model: "gpt-4",
        messages,
        temperature: 0.7,
      });

      res.json({
        message: response.choices[0].message,
        usage: response.usage,
      });
    } else if (provider === "claude") {
      response = await anthropic.messages.create({
        model: "claude-3-sonnet-20240229",
        messages,
        max_tokens: 500,
      });

      res.json({
        message: { role: "assistant", content: response.content[0].text },
        usage: response.usage,
      });
    }
  } catch (error) {
    res.status(500).json({ error: error.message });
  }
});

app.listen(3000, () => {
  console.log("Server running on port 3000");
});
```

## Session Management

### User-based Sessions

```javascript
import session from "express-session";

app.use(
  session({
    secret: process.env.SESSION_SECRET,
    resave: false,
    saveUninitialized: true,
  })
);

app.use(
  createExpressMiddleware({
    apiKey: process.env.LLMONITOR_API_KEY,
    extractSessionId: (req) => {
      // Use user ID if authenticated, otherwise session ID
      return req.user?.id || req.session.id;
    },
    extractMetadata: (req) => ({
      authenticated: !!req.user,
      userId: req.user?.id,
      userRole: req.user?.role,
    }),
  })
);
```

### Custom Session Logic

```javascript
class SessionManager {
  constructor() {
    this.sessions = new Map();
  }

  getOrCreateSession(req) {
    const sessionKey = this.extractSessionKey(req);

    if (!this.sessions.has(sessionKey)) {
      const sessionId = `${sessionKey}-${Date.now()}`;
      this.sessions.set(sessionKey, {
        id: sessionId,
        startTime: Date.now(),
        requestCount: 0,
      });
    }

    const session = this.sessions.get(sessionKey);
    session.requestCount++;
    session.lastActivity = Date.now();

    return session.id;
  }

  extractSessionKey(req) {
    // Custom logic to identify sessions
    return req.user?.id || req.ip || req.headers["x-device-id"];
  }
}

const sessionManager = new SessionManager();

app.use(
  createExpressMiddleware({
    apiKey: process.env.LLMONITOR_API_KEY,
    extractSessionId: (req) => sessionManager.getOrCreateSession(req),
  })
);
```

## Error Handling

### Comprehensive Error Handling

```javascript
app.use(
  createExpressMiddleware({
    apiKey: process.env.LLMONITOR_API_KEY,
    onError: (error, req, res) => {
      console.error("LLMonitor Error:", {
        error: error.message,
        stack: error.stack,
        path: req.path,
        method: req.method,
        timestamp: new Date().toISOString(),
      });

      // Optional: Send to external error tracking
      if (process.env.NODE_ENV === "production") {
        // errorTracker.captureException(error);
      }
    },
  })
);

// Global error handler for LLM operations
app.use(async (err, req, res, next) => {
  // Log LLM-related errors
  if (err.name === "OpenAIError" || err.name === "AnthropicError") {
    await monitor.logEvent({
      provider: err.provider || "unknown",
      model: err.model || "unknown",
      prompt: req.body?.message || "",
      completion: "",
      status: err.status || 500,
      latency_ms: Date.now() - req.startTime,
      session_id: req.session?.id,
      metadata: {
        error: err.message,
        endpoint: req.path,
      },
    });
  }

  res.status(500).json({
    error: "Internal server error",
    requestId: req.id,
  });
});
```

## Performance Monitoring

### Request Timing

```javascript
import { performance } from "perf_hooks";

app.use((req, res, next) => {
  req.startTime = performance.now();
  next();
});

app.use(
  createExpressMiddleware({
    apiKey: process.env.LLMONITOR_API_KEY,
    extractMetadata: (req) => ({
      requestDuration: performance.now() - req.startTime,
      endpoint: req.path,
      method: req.method,
    }),
  })
);
```

### Rate Limiting Integration

```javascript
import rateLimit from "express-rate-limit";

const apiLimiter = rateLimit({
  windowMs: 15 * 60 * 1000, // 15 minutes
  max: 100,
  message: "Too many requests",
  standardHeaders: true,
  legacyHeaders: false,
});

app.use("/api/", apiLimiter);

app.use(
  createExpressMiddleware({
    apiKey: process.env.LLMONITOR_API_KEY,
    extractMetadata: (req) => ({
      rateLimitRemaining: req.rateLimit?.remaining,
      rateLimitTotal: req.rateLimit?.limit,
    }),
  })
);
```

## Production Configuration

### Environment-based Setup

```javascript
// config/llmonitor.js
export const llmonitorConfig = {
  development: {
    apiKey: process.env.LLMONITOR_API_KEY,
    debug: true,
    enabled: true,
    skipPaths: ["/health"],
  },

  staging: {
    apiKey: process.env.LLMONITOR_API_KEY,
    debug: false,
    enabled: true,
    skipPaths: ["/health", "/metrics"],
    rateLimitPerMinute: 500,
  },

  production: {
    apiKey: process.env.LLMONITOR_API_KEY,
    debug: false,
    enabled: true,
    skipPaths: ["/health", "/metrics", "/internal"],
    rateLimitPerMinute: 1000,
    onError: (error) => {
      // Send to error tracking service
      console.error("LLMonitor production error:", error);
    },
  },
};

// app.js
import { llmonitorConfig } from "./config/llmonitor.js";

const config =
  llmonitorConfig[process.env.NODE_ENV] || llmonitorConfig.development;
app.use(createExpressMiddleware(config));
```

### Health Checks

```javascript
app.get("/health", (req, res) => {
  res.json({
    status: "healthy",
    timestamp: new Date().toISOString(),
    uptime: process.uptime(),
    llmonitor: {
      enabled: !!process.env.LLMONITOR_API_KEY,
      version: "1.0.0",
    },
  });
});

app.get("/metrics", async (req, res) => {
  // Return metrics that can be used by monitoring systems
  res.json({
    requests: requestCount,
    llm_calls: llmCallCount,
    errors: errorCount,
    average_response_time: averageResponseTime,
  });
});
```

## Testing

### Unit Tests

```javascript
import request from "supertest";
import { app } from "../app.js";

describe("LLM API Endpoints", () => {
  test("POST /api/chat/openai", async () => {
    const response = await request(app)
      .post("/api/chat/openai")
      .send({ message: "Hello!" })
      .expect(200);

    expect(response.body).toHaveProperty("reply");
    expect(response.body).toHaveProperty("model");
  });

  test("handles errors gracefully", async () => {
    // Mock OpenAI to throw an error
    const response = await request(app)
      .post("/api/chat/openai")
      .send({ message: "" })
      .expect(500);

    expect(response.body).toHaveProperty("error");
  });
});
```

### Integration Tests

```javascript
describe("LLMonitor Integration", () => {
  test("middleware tracks requests", async () => {
    const sessionId = "test-session-123";

    await request(app)
      .post("/api/chat/openai")
      .set("x-session-id", sessionId)
      .send({ message: "Test message" })
      .expect(200);

    // Verify that the request was logged to LLMonitor
    // (This would require access to LLMonitor's API or test endpoints)
  });
});
```

## Docker Configuration

```dockerfile
# Dockerfile
FROM node:18-alpine

WORKDIR /app

COPY package*.json ./
RUN npm ci --only=production

COPY . .

EXPOSE 3000

CMD ["node", "app.js"]
```

```yaml
# docker-compose.yml
version: "3.8"
services:
  api:
    build: .
    ports:
      - "3000:3000"
    environment:
      - NODE_ENV=production
      - LLMONITOR_API_KEY=${LLMONITOR_API_KEY}
      - OPENAI_API_KEY=${OPENAI_API_KEY}
      - ANTHROPIC_API_KEY=${ANTHROPIC_API_KEY}
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:3000/health"]
      interval: 30s
      timeout: 10s
      retries: 3
```

## Best Practices

1. **Environment Variables**: Store API keys securely
2. **Error Handling**: Never expose sensitive errors to clients
3. **Rate Limiting**: Protect your API from abuse
4. **Logging**: Use structured logging for better debugging
5. **Monitoring**: Set up health checks and metrics
6. **Security**: Validate inputs and sanitize outputs

```javascript
// Best practices example
import helmet from "helmet";
import cors from "cors";
import { body, validationResult } from "express-validator";

// Security middleware
app.use(helmet());
app.use(
  cors({
    origin: process.env.ALLOWED_ORIGINS?.split(",") || "*",
  })
);

// Input validation
app.post(
  "/api/chat/openai",
  [
    body("message").isLength({ min: 1, max: 4000 }).escape(),
    body("model").optional().isIn(["gpt-3.5-turbo", "gpt-4"]),
  ],
  async (req, res) => {
    const errors = validationResult(req);
    if (!errors.isEmpty()) {
      return res.status(400).json({ errors: errors.array() });
    }

    // Process request...
  }
);
```

## Next Steps

- [SDK Reference](/sdk) - Core SDK documentation
- [React Integration](/integrations/react) - Frontend integration
- [Cost Optimization](/guides/cost-optimization) - Reduce LLM costs
- [Error Handling Guide](/guides/error-handling) - Robust error handling
