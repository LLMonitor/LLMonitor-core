---
title: Welcome to LLMonitor
description: Model-agnostic LLM Observability & Cost Intelligence
---

import { Tab, Tabs } from "fumadocs-ui/components/tabs";
import { Step, Steps } from "fumadocs-ui/components/steps";

LLMonitor gives AI builders a **single pane of glass** to see **how every prompt behaves, how much it costs, and when it breaks**â€”across any model provider (OpenAI, Anthropic, Google, Cohere).

Replace home-grown logs, scattered dashboards and blind-spot debugging with an **actionable, dev-friendly analytics layer** that pays for itself by cutting token spend and downtime.

## ðŸ“¦ Available Packages

LLMonitor offers three packages to fit your development needs:

### @llmonitor/sdk

**Core SDK** - Works in any JavaScript/TypeScript environment

- âœ… Universal LLM provider wrappers (OpenAI, Anthropic, Google AI, Cohere)
- âœ… Automatic cost calculation and metrics tracking
- âœ… Express.js middleware
- âœ… Node.js, browser, and edge runtime support

### @llmonitor/react

**React Integration** - Hooks and components for React applications

- âœ… Context Provider for app-wide configuration
- âœ… `useLLMonitor` and `useLLMTracking` hooks
- âœ… `withLLMonitor` HOC for class components
- âœ… Reactive state management and session tracking

### @llmonitor/langchain (Coming Soon)

**LangChain Integration** - Native LangChain callback handlers

- âœ… Automatic chain, agent, and tool tracking
- âœ… LangChain Expression Language (LCEL) support
- âœ… Multi-step workflow observability

## Quick Start

<Steps>

<Step>

### Install the Package

Choose the package that fits your needs:

<Tabs items={['Core SDK', 'React', 'Both']}>

<Tab value="Core SDK">```bash npm install @llmonitor/sdk ```</Tab>

<Tab value="React">```bash npm install @llmonitor/react @llmonitor/sdk ```</Tab>

<Tab value="Both">```bash npm install @llmonitor/sdk @llmonitor/react ```</Tab>

</Tabs>

</Step>

<Step>

### Core SDK Usage

```typescript
import { LLMonitor } from "@llmonitor/sdk";
import OpenAI from "openai";

const monitor = new LLMonitor({
  apiKey: "your-api-key", // Get this from your dashboard
  sessionId: "user-session-123",
  versionTag: "v1.0.0",
});

// Wrap any LLM client
const openai = monitor.openai(new OpenAI({ apiKey: "your-openai-key" }));

// Use exactly like normal - automatically logged!
const response = await openai.chat.completions.create({
  model: "gpt-4",
  messages: [{ role: "user", content: "Hello!" }],
});
```

</Step>

<Step>

### React Usage

```jsx
import { LLMonitorProvider, useLLMonitor } from "@llmonitor/react";

// Wrap your app
function App() {
  return (
    <LLMonitorProvider config={{ apiKey: "your-api-key" }}>
      <ChatComponent />
    </LLMonitorProvider>
  );
}

// Use in components
function ChatComponent() {
  const { track, logEvent, sessionId, setSessionId } = useLLMonitor();

  const handleAICall = async () => {
    await track('chat-completion', async () => {
      // Your LLM logic here
      return await openai.chat.completions.create({...});
    });
  };
}
```

</Step>

</Steps>

## Supported Providers

| Provider      | SDK Support | Pricing            | Models                   |
| ------------- | ----------- | ------------------ | ------------------------ |
| **OpenAI**    | âœ… Full     | âœ… Auto-calculated | GPT-4, GPT-3.5, etc.     |
| **Anthropic** | âœ… Full     | âœ… Auto-calculated | Claude-3, Claude-3.5     |
| **Google AI** | âœ… Full     | âœ… Auto-calculated | Gemini Pro, Gemini Flash |
| **Cohere**    | âœ… Full     | âœ… Auto-calculated | Command, Command-R       |

## Key Features

### ðŸ“Š **Centralized Observability**

- All LLM calls across providers in one dashboard
- Real-time metrics and historical trends
- Session-based request grouping

### ðŸ’° **Cost Intelligence**

- Automatic cost calculation for all providers
- Token usage optimization insights
- Budget alerts and spend tracking

### âš¡ **Performance Monitoring**

- Latency tracking and P95/P99 metrics
- Success rate monitoring
- Error categorization and debugging

### ðŸ”§ **Developer Experience**

- Drop-in replacement for existing LLM clients
- TypeScript-first with full type safety
- Zero-config setup with sensible defaults

### ðŸš¨ **Smart Alerting**

- Cost threshold notifications
- Error rate alerts
- Performance degradation detection

## Architecture

```
Your App â”€â”€â†’ @llmonitor/react â”€â”€â†’ @llmonitor/sdk â”€â”€â†’ LLMonitor API
                                      â†—
         â”€â”€â†’ @llmonitor/langchain â”€â”€â”€â”€â†’
```

## What's Next?

- [Getting Started Guide](/guides/getting-started) - Complete setup tutorial
- [SDK Reference](/sdk) - Full API documentation
- [React Integration](/integrations/react) - React-specific features
- [Provider Examples](/integrations) - Implementation guides for each LLM provider
- [Cost Optimization](/guides/cost-optimization) - Save money on LLM usage
