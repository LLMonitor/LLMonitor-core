---
title: Getting Started
description: Complete setup guide from installation to first metrics
---

import { Tab, Tabs } from "fumadocs-ui/components/tabs";
import { Step, Steps } from "fumadocs-ui/components/steps";
import { Callout } from "fumadocs-ui/components/callout";

# Getting Started

This guide will walk you through setting up LLMonitor in your application from scratch.

## Prerequisites

Before you begin, make sure you have:

- A Node.js application (version 16 or higher)
- An API key from your LLMonitor dashboard
- An LLM provider (OpenAI, Anthropic, etc.)

<Callout type="info">
  Don't have an API key yet? Sign up at [llmonitor.ai](https://llmonitor.ai) to
  get started for free.
</Callout>

## Installation

<Steps>

<Step>

### Install the SDK

Install the LLMonitor SDK in your project:

<Tabs items={['npm', 'yarn', 'pnpm', 'bun']}>

<Tab value="npm">```bash npm install @llmonitor/sdk ```</Tab>

<Tab value="yarn">```bash yarn add @llmonitor/sdk ```</Tab>

<Tab value="pnpm">```bash pnpm add @llmonitor/sdk ```</Tab>

<Tab value="bun">```bash bun add @llmonitor/sdk ```</Tab>

</Tabs>

</Step>

<Step>

### Initialize LLMonitor

Create a new instance of LLMonitor with your API key:

```typescript
import { LLMonitor } from "@llmonitor/sdk";

const monitor = new LLMonitor({
  apiKey: process.env.LLMONITOR_API_KEY, // Your API key
  sessionId: "user-session-123", // Optional: track user sessions
  versionTag: "v1.0.0", // Optional: tag your deployments
  environment: "production", // Optional: separate dev/staging/prod
});
```

<Callout type="warn">
  Store your API key securely in environment variables, never commit it to your
  code.
</Callout>

</Step>

<Step>

### Set up environment variables

Create a `.env` file in your project root:

```bash
LLMONITOR_API_KEY=your_api_key_here
OPENAI_API_KEY=your_openai_key_here
```

</Step>

<Step>

### Wrap your LLM client

Wrap your existing LLM client with LLMonitor:

<Tabs items={['OpenAI', 'Anthropic', 'Google']}>

<Tab value="OpenAI">
```typescript
import OpenAI from "openai";

const openai = new OpenAI({
apiKey: process.env.OPENAI_API_KEY,
});

// Wrap with LLMonitor
const monitoredOpenAI = monitor.openai(openai);

// Use exactly like normal OpenAI - automatically logged!
const response = await monitoredOpenAI.chat.completions.create({
model: "gpt-4",
messages: [{ role: "user", content: "Hello!" }],
});

````
</Tab>

<Tab value="Anthropic">
```typescript
import { Anthropic } from "@anthropic-ai/sdk";

const anthropic = new Anthropic({
  apiKey: process.env.ANTHROPIC_API_KEY,
});

// Wrap with LLMonitor
const monitoredAnthropic = monitor.anthropic(anthropic);

const response = await monitoredAnthropic.messages.create({
  model: "claude-3-sonnet-20240229",
  max_tokens: 1000,
  messages: [{ role: "user", content: "Hello!" }],
});
````

</Tab>

<Tab value="Google">
```typescript
import { GoogleGenerativeAI } from "@google/generative-ai";

const genAI = new GoogleGenerativeAI(process.env.GOOGLE_API_KEY);
const model = genAI.getGenerativeModel({ model: "gemini-pro" });

// Wrap with LLMonitor
const monitoredModel = monitor.google(model);

const response = await monitoredModel.generateContent("Hello!");

````
</Tab>

</Tabs>

</Step>

<Step>

### Test your setup

Run a simple test to verify everything is working:

```typescript
async function testLLMonitor() {
  try {
    const response = await monitoredOpenAI.chat.completions.create({
      model: "gpt-3.5-turbo",
      messages: [
        { role: "system", content: "You are a helpful assistant." },
        { role: "user", content: "Say hello and confirm LLMonitor is working!" }
      ],
      max_tokens: 50,
    });

    console.log("Response:", response.choices[0]?.message.content);
    console.log("✅ LLMonitor is working!");
  } catch (error) {
    console.error("❌ Error:", error);
  }
}

testLLMonitor();
````

</Step>

</Steps>

## Verify Your Setup

After running your test, check your LLMonitor dashboard:

1. **Go to your dashboard** at [llmonitor.ai/dashboard](https://llmonitor.ai/dashboard)
2. **Check the "Recent Calls" section** - you should see your test call
3. **Verify the metrics** - cost, tokens, latency should be displayed

<Callout type="success">
  If you see your test call in the dashboard, congratulations! LLMonitor is
  successfully tracking your LLM usage.
</Callout>

## Next Steps

Now that LLMonitor is set up, explore these features:

- **[Cost Optimization](/docs/guides/cost-optimization)** - Learn how to reduce your LLM costs
- **[Session Tracking](/docs/guides/sessions)** - Track user conversations and context
- **[A/B Testing](/docs/guides/ab-testing)** - Compare different prompts and models
- **[Error Handling](/docs/guides/error-handling)** - Monitor and handle LLM errors
- **[Performance Monitoring](/docs/guides/performance)** - Optimize your LLM performance

## Common Issues

### API Key Not Working

Make sure your API key is correct and has the proper permissions:

```typescript
// Test your API key
const monitor = new LLMonitor({
  apiKey: "your-api-key",
  debug: true, // Enable debug mode to see what's happening
});
```

### Calls Not Appearing in Dashboard

1. Check that your API key is set correctly
2. Verify you're wrapping the correct client instance
3. Ensure your firewall allows outbound HTTPS connections
4. Check the browser console for any JavaScript errors

### Performance Issues

LLMonitor adds minimal overhead (~1-2ms per call), but if you experience issues:

1. Enable async mode for fire-and-forget logging
2. Use batch mode for high-volume applications
3. Configure appropriate timeout settings

```typescript
const monitor = new LLMonitor({
  apiKey: "your-api-key",
  async: true, // Fire-and-forget logging
  batchSize: 10, // Batch multiple calls
  timeout: 5000, // 5 second timeout
});
```

## Need Help?

- Check our [Troubleshooting Guide](/docs/guides/troubleshooting)
- Join our [Discord Community](https://discord.gg/llmonitor)
- Email us at [support@llmonitor.ai](mailto:support@llmonitor.ai)
