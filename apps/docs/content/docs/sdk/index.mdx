---
title: SDK Reference
description: Complete API reference for @llmonitor/sdk
---

import { Tab, Tabs } from "fumadocs-ui/components/tabs";
import { Step, Steps } from "fumadocs-ui/components/steps";

The **@llmonitor/sdk** is the core package that provides universal LLM monitoring across all major providers. It works in any JavaScript/TypeScript environment.

## Installation

<Tabs items={['npm', 'yarn', 'pnpm']}>

<Tab value="npm">```bash npm install @llmonitor/sdk ```</Tab>

<Tab value="yarn">```bash yarn add @llmonitor/sdk ```</Tab>

<Tab value="pnpm">```bash pnpm add @llmonitor/sdk ```</Tab>

</Tabs>

## Core Configuration

```typescript
import { LLMonitor } from "@llmonitor/sdk";

const monitor = new LLMonitor({
  apiKey: "your-api-key", // Required: Get from dashboard
  baseURL: "https://api.llmonitor.com", // Optional: Custom endpoint
  sessionId: "user-123", // Optional: Session tracking
  versionTag: "v1.0.0", // Optional: Version tracking
  debug: false, // Optional: Enable debug logs
  enabled: true, // Optional: Disable monitoring
  metadata: {
    // Optional: Additional metadata
    environment: "production",
    userId: "user-123",
  },
});
```

## Provider Integrations

### OpenAI

```typescript
import OpenAI from "openai";

const openai = monitor.openai(
  new OpenAI({
    apiKey: "your-openai-key",
  })
);

// All methods work exactly the same
const response = await openai.chat.completions.create({
  model: "gpt-4",
  messages: [{ role: "user", content: "Hello!" }],
  temperature: 0.7,
  max_tokens: 150,
});
```

**Supported OpenAI Features:**

- ✅ Chat completions
- ✅ Automatic cost calculation
- ✅ Token counting
- ✅ Error tracking
- ✅ All GPT models (GPT-4, GPT-3.5, etc.)

### Anthropic

```typescript
import { Anthropic } from "@anthropic-ai/sdk";

const anthropic = monitor.anthropic(
  new Anthropic({
    apiKey: "your-anthropic-key",
  })
);

const response = await anthropic.messages.create({
  model: "claude-3-sonnet-20240229",
  messages: [{ role: "user", content: "Hello!" }],
  max_tokens: 150,
});
```

**Supported Anthropic Features:**

- ✅ Message creation
- ✅ Automatic cost calculation
- ✅ All Claude models (Claude-3, Claude-3.5)
- ✅ Token usage tracking

### Google AI

```typescript
import { GoogleGenerativeAI } from "@google/generative-ai";

const genAI = new GoogleGenerativeAI("your-google-key");
const model = genAI.getGenerativeModel({ model: "gemini-pro" });
const monitoredModel = monitor.google(model, "gemini-pro");

// Regular generation
const response = await monitoredModel.generateContent("Hello!");

// Streaming
const streamResponse = await monitoredModel.generateContentStream("Hello!");
```

**Supported Google AI Features:**

- ✅ Content generation
- ✅ Streaming support
- ✅ All Gemini models
- ✅ Automatic cost calculation

### Cohere

```typescript
import { CohereClient } from "cohere-ai";

const cohere = monitor.cohere(
  new CohereClient({
    token: "your-cohere-key",
  })
);

// Generate text
const generateResponse = await cohere.generate({
  prompt: "Hello!",
  model: "command",
  max_tokens: 150,
});

// Chat
const chatResponse = await cohere.chat({
  message: "Hello!",
  model: "command",
});
```

**Supported Cohere Features:**

- ✅ Text generation
- ✅ Chat completions
- ✅ All Command models
- ✅ Automatic cost calculation

## Express.js Middleware

```typescript
import express from "express";
import { createExpressMiddleware } from "@llmonitor/sdk";

const app = express();

// Add middleware
app.use(
  createExpressMiddleware({
    apiKey: "your-api-key",
    skipPaths: ["/health", "/metrics"],
    extractSessionId: (req) => req.headers["x-session-id"],
    extractMetadata: (req) => ({
      userAgent: req.headers["user-agent"],
      ip: req.ip,
    }),
  })
);
```

## API Reference

### LLMonitor Class

#### Constructor

```typescript
new LLMonitor(config: LLMonitorConfig)
```

#### Methods

| Method                  | Description               | Returns            |
| ----------------------- | ------------------------- | ------------------ |
| `openai(client)`        | Wrap OpenAI client        | `OpenAIWrapper`    |
| `anthropic(client)`     | Wrap Anthropic client     | `AnthropicWrapper` |
| `google(model, name?)`  | Wrap Google AI model      | `GoogleWrapper`    |
| `cohere(client)`        | Wrap Cohere client        | `CohereWrapper`    |
| `express(options?)`     | Create Express middleware | `Middleware`       |
| `logEvent(event)`       | Log custom event          | `Promise<void>`    |
| `flush()`               | Flush pending events      | `Promise<void>`    |
| `updateConfig(updates)` | Update configuration      | `void`             |

### LLMonitorConfig

```typescript
interface LLMonitorConfig {
  apiKey: string; // Required
  baseURL?: string; // Default: "http://localhost:3001"
  sessionId?: string; // Optional session ID
  versionTag?: string; // Optional version tag
  debug?: boolean; // Default: false
  enabled?: boolean; // Default: true
  metadata?: Record<string, any>; // Optional metadata
}
```

### LLMEvent

```typescript
interface LLMEvent {
  provider: string; // Provider name
  model: string; // Model name
  prompt: string; // Input prompt
  completion: string; // Model response
  temperature?: number; // Temperature setting
  max_tokens?: number; // Max tokens setting
  prompt_tokens?: number; // Input tokens used
  completion_tokens?: number; // Output tokens used
  latency_ms?: number; // Request latency
  status: number; // HTTP status code
  cost_usd?: number; // Calculated cost
  session_id?: string; // Session identifier
  request_id?: string; // Request identifier
  version_tag?: string; // Version tag
  metadata?: Record<string, any>; // Additional metadata
}
```

## Cost Calculation

The SDK automatically calculates costs for all supported providers using current pricing:

| Provider            | Input Cost (per 1M tokens) | Output Cost (per 1M tokens) |
| ------------------- | -------------------------- | --------------------------- |
| **GPT-4**           | $30.00                     | $60.00                      |
| **GPT-4 Turbo**     | $10.00                     | $30.00                      |
| **GPT-3.5 Turbo**   | $1.00                      | $2.00                       |
| **Claude-3 Opus**   | $15.00                     | $75.00                      |
| **Claude-3 Sonnet** | $3.00                      | $15.00                      |
| **Gemini Pro**      | $0.50                      | $1.50                       |
| **Command**         | $15.00                     | $15.00                      |

## Error Handling

The SDK gracefully handles errors and continues working even when monitoring fails:

```typescript
try {
  const response = await monitoredOpenAI.chat.completions.create({...});
  // Your response
} catch (error) {
  // Original LLM error is preserved
  console.error("LLM call failed:", error);
}
```

## TypeScript Support

The SDK is built with TypeScript and provides full type safety:

```typescript
import { LLMonitor, LLMEvent, ProviderOptions } from "@llmonitor/sdk";

// All types are exported and available
const event: LLMEvent = {
  provider: "openai",
  model: "gpt-4",
  prompt: "Hello",
  completion: "Hi there!",
  status: 200,
};
```

## Environment Variables

You can use environment variables for configuration:

```bash
LLMONITOR_API_KEY=your-api-key
LLMONITOR_BASE_URL=https://api.llmonitor.com
LLMONITOR_DEBUG=false
```

```typescript
// Will automatically use environment variables
const monitor = new LLMonitor({
  apiKey: process.env.LLMONITOR_API_KEY!,
});
```

## Best Practices

1. **Single Instance**: Create one LLMonitor instance per application
2. **Session Tracking**: Use consistent session IDs for user tracking
3. **Error Handling**: Always wrap LLM calls in try/catch
4. **Metadata**: Add relevant metadata for better debugging
5. **Flush on Exit**: Call `flush()` before application shutdown

```typescript
// Good pattern
const monitor = new LLMonitor({
  apiKey: process.env.LLMONITOR_API_KEY!,
  sessionId: req.session.id,
  metadata: { userId: req.user.id },
});

// Graceful shutdown
process.on("SIGTERM", async () => {
  await monitor.flush();
  process.exit(0);
});
```
