# Observability for your AI stack

## Not just GPT. We support Claude, Gemini, Grok, and beyond.

LLMFlow helps you understand, optimize, and debug how your app uses large language models — across providers.

---

### 🔍 What you get

- **Prompt analytics**  
  Track every request: input, output, latency, status, token usage.

- **Multi-model support**  
  Works out of the box with OpenAI, Claude, Gemini, Mistral, Grok, or your own hosted model.

- **Cost & performance visibility**  
  Know which prompts cost the most, perform the worst, or fail silently.

- **Prompt drift detection**  
  Catch behavioral changes in outputs over time or across providers.

- **A/B testing engine**  
  Test prompt versions or model configs side by side with real users.

- **Slack & webhook alerts**  
  Get notified when something breaks or becomes too expensive.

---

### ⚙️ Built for developers

- 1-line integration via SDK or proxy
- Simple API for sending logs
- Zero vendor lock-in
- Fast setup. Works with LangChain, LlamaIndex, and raw API calls.

---

### 🧑‍💻 Who is this for?

- AI product teams
- LLM-heavy startups
- Tool builders and AI agents
- Anyone burning tokens and debugging in the dark

---

### 🚀 Join the waitlist

We’re onboarding early users.

👉 [Sign up here](#)

---
